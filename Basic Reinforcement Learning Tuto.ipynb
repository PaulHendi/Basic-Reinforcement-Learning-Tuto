{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple first introduction to reinforcement learning \n",
    "<br>\n",
    "\n",
    "### Briefly Reinforcement Learning is : \n",
    "<br>\n",
    "In an environnement E, an agent in a state S takes an action to go to another state S' and receive a reward R.\n",
    "The choice of the action to take is defined by a policy P which that defines for each state what is the best action.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "To illustrate this simply, we will use a simple use case : a robot that has to navigate to reach a goal avoiding fire that can damage him.\n",
    "    \n",
    "\n",
    "<img src=\"img/GridRL.png\" width=\"60%\">\n",
    "\n",
    "<br>\n",
    "Although this environnement doesn't have so many states, 9 only, the number of possible actions depending on the\n",
    "current state is actually high. We have to define by hand the reward for taking an action in each state, and whenever it is not possible, it is defined by Nan.\n",
    "\n",
    "<img src=\"img/ArrayPossibleActionsRL.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace Nan by -100 like if it is a big punishment\n",
    "R = [[0,-100,-100,0],[0,-100,0,0],[-1,-100,0,-100],[1,0,0,-100],[0,0,0,-1],[-1,0,-100,0],[-100,0,-100,0],[-100,0,-1,1],[-100,-1,0,-100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating through the environement\n",
    "\n",
    "Now we need to compute the policy P that doesn't only take into account the current reward but future rewards as well. We will use a well-known method : Q-learning. \n",
    "<br>\n",
    "\n",
    "The Q-learning method gives us a Q(s,a) function representing the value (function of reward) of taking an action a in state s assuming that the agent follows the policy. The agent searching for the optimal policy can then be described using the Bellman optimality equation as : \n",
    "<br>\n",
    "\n",
    "$$ Q^{*}(s_{t}, a_{t}) = r(s_{t+1}, a_{t+1}) + \\gamma \\times \\max_{a_{t+1}}Q^{*}(s_{t+1}, a_{t+1}) $$ \n",
    "<br>\n",
    "Which means the Q-value at time t equals the sum of the immediate reward and the estimate of optimal future value.\n",
    "The parameter $ \\gamma $ is called the discount factor, it gives more importance to close rewards than future reward.\n",
    "\n",
    "<br>\n",
    "A common way of choosing which action to take is the $ \\epsilon $-greedy strategy. Instead of always taking the best action, we can force the agent to fully explore the environement by taking random actions with probability $ \\epsilon $, and best actions with probability $ 1 - \\epsilon $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to programming we can initialize the Q look-up table\n",
    "Q = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "\n",
    "# We also define an array that maps action to states to know in which state when taking an action \n",
    "# 0 means that is action is not possible because the next state doesn't exist\n",
    "ActionToState = [[6,0,0,2], [5,0,1,3],[4,0,2,0],[9,3,5,0],[8,2,6,4],[7,1,0,5],[0,6,0,8],[0,5,7,9],[0,4,8,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameter \n",
    "gamma = 0.7\n",
    "\n",
    "# n_episode correspond to number of times the robot will try to catch the diamond\n",
    "n_episode = 10000  \n",
    "next_state = 0\n",
    "\n",
    "for i in range(n_episode) : \n",
    "\n",
    "    # Initilialize current state and nb of try at each start of an episode\n",
    "    current_state = 0\n",
    "    nb_try = 0\n",
    "\n",
    "    \n",
    "    # An episode stops if the robots reached the diamond or if he's been wandering too much\n",
    "    while (state!=8 or nb_try<50) :\n",
    "\n",
    "        # In this loop the robot takes an action, it the action is not possible then he tries again.\n",
    "        # Also the action is taken following the epsilon greedy strategy to fully explore the environement \n",
    "        while True :\n",
    "            \n",
    "            epsilon = np.random.random()\n",
    "            \n",
    "            if (epsilon>0.3) : \n",
    "                \n",
    "                # Greedy choice of action => Good for exploration\n",
    "                action = np.argmax(Q[current_state][:])  \n",
    "            \n",
    "            else :  \n",
    "            \n",
    "                # Random choice of action\n",
    "                action = int(round(np.random.random()*3)) \n",
    "                \n",
    "            # We compute the next_state from the array defined above\n",
    "            next_state = ActionToState[current_state][action] -1\n",
    "            \n",
    "            # If the action is not feasible then the robot has to take antoher action\n",
    "            if (next_state!=-1) : break\n",
    "\n",
    "        # Q value is updated with the sum of : \n",
    "        #     _ current reward defined by the R array according to current state and action\n",
    "        #     _ the discounted future Q value according to next state\n",
    "        Q[current_state][action] = R[current_state][action] + gamma*max(Q[next_state][:])\n",
    "\n",
    "        \n",
    "        # Update current state and nb of try for next iteration\n",
    "        current_state = next_state\n",
    "        nb_try += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Up\n",
      "2 : Up\n",
      "3 : Left\n",
      "4 : Up\n",
      "5 : Up\n",
      "6 : Right\n",
      "7 : Right\n",
      "8 : Right\n",
      "9 : Left\n"
     ]
    }
   ],
   "source": [
    "# We can print what is the best action depending on each current state\n",
    "\n",
    "# First let's define a dict mapping an index to an action\n",
    "Index2Action = {0 : \"Up\", 1 : \"Down\", 2 : \"Left\", 3 : \"Right\"}\n",
    "\n",
    "# Then we can just loop through each state and print the argmax of the Q value along each line \n",
    "count = 1\n",
    "for i in range(9) :\n",
    "    BestAction = Index2Action[np.argmax(Q[i][:])]\n",
    "    print(str(count) + \" : \" + BestAction)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
